{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNo4/DW813DwL1e46qIgSzw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimgeonhee317/stable-diffusion-lec-notes/blob/main/notebook/lec_note.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Stable diffusion basics"
      ],
      "metadata": {
        "id": "_zCOXiC_TfWc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Types of Image generation\n",
        "1. Unconditional image generation: model just generates images without any additional condition like text and image. You will get images similar to provided in the training set.\n",
        "\n",
        "2. Based on text : text to image, text2img\n",
        "The prompt is converted to embedding\n",
        "\n",
        "3. Based on image : image to image, img2img"
      ],
      "metadata": {
        "id": "ESNVBEMaTyxf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Diffusion Model\n",
        "1. Diffusion models are essentially Markov chains trained using variational inference.\n",
        "2. The purpose of diffusion models is to learn the latent structure of a dataset by modeling the way data points diffuse through the latent space\n",
        "3. In the field of computer vision, this means that a neural network is trained to reduce noise from images blurred with Gaussian noise by learning how to reverse the diffusion process.\n",
        "4. In other words, it is the process that will transform noise (the initial state) into generated image (the result)"
      ],
      "metadata": {
        "id": "CYv1FMp2YMN2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Autoencoders\n",
        "Intorduced as a powerful tool to compress images or data in general.\n",
        "- Encoder: converts the input into lower-dimensional latent vector, usually through standard convolutions and clustering layers.\n",
        "- Decoder: performs operations such as deconvolution and upsampling to try to reconstruct the input image"
      ],
      "metadata": {
        "id": "Nfh0i1zIYWRI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variational autoencoders\n",
        "+ The encoder returns the mean and the standard deviation for each input; then, the latent vector is sampled from this distribution and sent to the decoder to reconstruct the input\n",
        "+ Training is now controlled by reconstruction loss and similarity loss, which is the KL divergence between the latent space distribution and the standard Gaussian.\n",
        "+ VAEs are trained not only to reconstruct images, but also to produce latent vectors from a normal distribution"
      ],
      "metadata": {
        "id": "M150ALl0cZiF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### U-NET\n",
        "+ encoder: the first part and it is usually a pre-trained classification network (like VGG/ResNet).\n",
        "Convolution blocks followed by maxpool/downsampling are applied to encode the input image into feature representations at several different levels\n",
        "\n",
        "+ decoder: it is the second part of the architecture, which has the goal to semantically project the discriminative features (lower resolution) learned by the encoder into pixel space (higher resolution) to obtain a dense classification. The decoder consists of upsampling and concatenation followed by regular convolution operations."
      ],
      "metadata": {
        "id": "EFpbR-d9dGvc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text-encoder\n",
        "+ The text-encoder is responsible for turning the input prompt into a embedding space that can be understood by U-net.\n",
        "+ It is usally a simple transformer=based encoder that maps a squence of input tokens to a sequence of latent text-embeddings.\n",
        "+ Inspired by Imagen, Stable diffusion does not train the text encoder during training, it simply uses an already trained CLIP text encodert, the **CLIPTextModel**"
      ],
      "metadata": {
        "id": "bI4UVla2epst"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stable diffusion inference\n",
        "+ SD receives a latent seed and a text prompt as input\n",
        "+ The seed is used to generate random represntations of latent images of size 64 * 64 (4096)\n",
        "+ The text prompt is transformed into text embeddings of size 77 * 768 using CLIP text encoder. (Maximum sequence word numbers is 77)\n",
        "+ U-net iteratively reduces noise from the random latent image representations while conditioning on the text embeddings. => Text conditioned latent Unet\n",
        "+ The U-Net output(noise residual) is used to compute a denoised latent image representation using a scheduler algorithm.\n",
        "+ The denoising process is prepeated x times to recover the best latent image representations.\n",
        "+ Once completed, the latent image representation is decoded by the decoder part of the VAE\n",
        "\n"
      ],
      "metadata": {
        "id": "ug3dhzWkgeqw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Recap Components\n",
        "+ text encoder: Stable diffusion uses CLIP, but other diffusion models can use other encoders like BERT.\n",
        "+ tokenizer: It must match the one used by the text_encoder model.\n",
        "+ scheduler: The scheduler algorithm used to progressively add noise to the image during training\n",
        "+ u-net: The model used to generate the latent representation of the input.\n",
        "+ vae: Autoencoder module that we will use to decode latentrepresentations into real images.\n"
      ],
      "metadata": {
        "id": "fv2GaMbykazV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scheduler algorithms\n",
        "+ Scheduler algorithms (also called samplers) calculates the predicted denosied image representation from the previous noise representation and the predicted residual noise.\n",
        "+ Determines how the image is calculated.\n",
        "+ There are several different algorithms. Some examples commonly used with stable diffusion.\n",
        "  + PNDM (default)\n",
        "  + DDIM scheduler\n",
        "  + K-LMS scheduler\n",
        "  + Euler Ancestral Discrete Scheduler\n",
        "  + DPM Scheduler\n",
        "\n"
      ],
      "metadata": {
        "id": "vh9o70AylV0g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "LuXFXneNmcS_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GTuHpBl5ctrY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}